# Robots.txt for Utsav Travels
# This file tells search engine crawlers which pages to index

# Allow all web crawlers access to all content
User-agent: *
Allow: /

# Specifically allow Google, Bing, and other major search engines
User-agent: Googlebot
Allow: /

User-agent: Bingbot
Allow: /

User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

# Disallow crawling of admin, API, and private areas
Disallow: /admin/
Disallow: /api/
Disallow: /_next/
Disallow: /private/

# Disallow crawling of duplicate or low-value pages
Disallow: /*?utm_*
Disallow: /*?fb_*
Disallow: /*?gclid=*
Disallow: /search?*
Disallow: /*?sort=*
Disallow: /*?filter=*

# Allow crawling of important pages
Allow: /places/
Allow: /packages/
Allow: /about
Allow: /contact

# Crawl delay (be respectful to server resources)
Crawl-delay: 1

# Sitemap location
Sitemap: https://utsavtravels.com/sitemap.xml

# Additional sitemaps for better organization
Sitemap: https://utsavtravels.com/sitemap-places.xml
Sitemap: https://utsavtravels.com/sitemap-packages.xml
Sitemap: https://utsavtravels.com/sitemap-static.xml